{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import codecs\n",
    "\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading connlu dataset file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_connlu(file):\n",
    "    words, tags, sent_words, sent_tags = [],[],[],[]\n",
    "    with codecs.open(file, 'r', 'utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if len(line) == 0 or line.startswith('-DOCSTART-'):\n",
    "            if len(sent_words) > 0:\n",
    "                words.append(sent_words)\n",
    "                tags.append(sent_tags)\n",
    "                sent_words,sent_tags = [],[]\n",
    "            continue\n",
    "        line_splitted = line.split(' ')\n",
    "        sent_words.append(line_splitted[0])\n",
    "        sent_tags.append(line_splitted[-1])\n",
    "    if len(sent_words) > 0:\n",
    "        words.append(sent_words)\n",
    "        tags.append(sent_tags)     \n",
    "    return np.array(words), np.array(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_embeddings():\n",
    "    embeddings = {}\n",
    "    with codecs.open('glove.6B.100d.txt', 'r', 'utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line_splitted = line.strip().split(\" \")\n",
    "        word = line_splitted[0]\n",
    "        embeddings[word] = np.array(line_splitted[1:]).astype(np.float)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = read_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words, train_tags = read_connlu(\"train.txt\")\n",
    "dev_words, dev_tags = read_connlu(\"dev.txt\")\n",
    "test_words, test_tags = read_connlu(\"test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_to_ix = {'B-LOC':0, 'B-MISC':1, 'B-ORG':2, 'B-PER':3, 'I-LOC':4, 'I-MISC':5, 'I-ORG':6, 'I-PER':7, 'O':8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_WORD = \"<pad>\"\n",
    "word_to_ix = {}\n",
    "for sentences in [train_words, dev_words, test_words]:\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            if word not in word_to_ix:\n",
    "                word_to_ix[word] = len(word_to_ix)\n",
    "word_to_ix[PAD_WORD] = len(word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "EMBEDDING_DIM = len(embeddings['.'])\n",
    "BATCH_SIZE = 128\n",
    "TARGET_SIZE = len(tag_to_ix)\n",
    "VOCAB_SIZE = len(word_to_ix)\n",
    "HIDDEN_DIM = 120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lowercase_embedding_matrix():\n",
    "    embeddings_matrix = np.empty((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "    for word, index in word_to_ix.items():\n",
    "        word_lower_embedding = embeddings.get(word.lower())\n",
    "        embeddings_matrix[index] = word_lower_embedding if word_lower_embedding  is not None else embeddings['unknown']\n",
    "    return embeddings_matrix\n",
    "        \n",
    "def get_original_embedding_matrix():\n",
    "    embeddings_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "    for word, index in word_to_ix.items():\n",
    "        original_word_embedding = embeddings.get(word)\n",
    "        embeddings_matrix[index] = original_word_embedding if original_word_embedding is not None else embeddings['unknown']\n",
    "    return embeddings_matrix\n",
    "\n",
    "def get_full_embedding_matrix():\n",
    "    embeddings_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "    for word, index in word_to_ix.items():\n",
    "        original_word_embedding = embeddings.get(word)\n",
    "        word_lower_embedding = embeddings.get(word.lower())\n",
    "        if original_word_embedding is not None:\n",
    "            embeddings_matrix[index] = original_word_embedding\n",
    "        elif word_lower_embedding  is not None:\n",
    "            embeddings_matrix[index] = word_lower_embedding\n",
    "        else:\n",
    "            embeddings_matrix[index] = embeddings['unknown']\n",
    "    return embeddings_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequential models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim).from_pretrained(\n",
    "            torch.tensor(get_original_embedding_matrix(), dtype=torch.float))\n",
    "        self.word_embeddings.weight.requires_grad = False\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        \n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n",
    "    \n",
    "class BiLSTM_TaggerIter(nn.Module):\n",
    "    def __init__(self, embedding_dim, vocab_size, tagset_size,\n",
    "                 hidden_dim=128, embedding_type='original'):\n",
    "        super(BiLSTM_Tagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        if embedding_type == \"original\":\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim)\\\n",
    "                .from_pretrained(torch.tensor(get_original_embedding_matrix(), dtype=torch.float))\n",
    "        elif embedding_type == \"lower\":\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim)\\\n",
    "                .from_pretrained(torch.tensor(get_lowercase_embedding_matrix(), dtype=torch.float))\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim)\\\n",
    "                .from_pretrained(torch.tensor(get_full_embedding_matrix(), dtype=torch.float))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.lstm = nn.LSTM(input_size=self.embedding.embedding_dim,\n",
    "                            hidden_size=hidden_dim,\n",
    "                            num_layers=1,\n",
    "                            bidirectional=True)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim*2, tagset_size)\n",
    "    \n",
    "    def forward(self, sents):\n",
    "        x = self.embedding(sents)\n",
    "        x = x.view(len(sents), 1, -1)\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        tags_output = self.hidden2tag(lstm_out)\n",
    "        tags_prob = F.log_softmax(tags_output, dim=1)\n",
    "        return tags_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models for batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_Tagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, vocab_size, tagset_size,\n",
    "                 hidden_dim=128, embedding_type='original'):\n",
    "        super(BiLSTM_Tagger, self).__init__()\n",
    "        self.target_size = tagset_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        if embedding_type == \"original\":\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim)\\\n",
    "                .from_pretrained(torch.tensor(get_original_embedding_matrix(), dtype=torch.float))\n",
    "        elif embedding_type == \"lower\":\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim)\\\n",
    "                .from_pretrained(torch.tensor(get_lowercase_embedding_matrix(), dtype=torch.float))\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim)\\\n",
    "                .from_pretrained(torch.tensor(get_full_embedding_matrix(), dtype=torch.float))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.lstm = nn.LSTM(input_size=self.embedding.embedding_dim,\n",
    "                            hidden_size=hidden_dim,\n",
    "                            num_layers=1,\n",
    "                            bidirectional=True)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim*2, tagset_size)\n",
    "    \n",
    "    def forward(self, sents):\n",
    "        x = self.embedding(sents)\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        flatten = lstm_out.view(-1, lstm_out.shape[2]) \n",
    "        tags_output = self.hidden2tag(flatten)\n",
    "        tags_prob = F.log_softmax(tags_output, dim=1)\n",
    "        return tags_prob\n",
    "\n",
    "    def loss_function(self, outputs, labels):\n",
    "        # reshape labels to give a flat vector of length batch_size*seq_len\n",
    "        labels = labels.view(-1)\n",
    "        mask = (labels >= 0).float()\n",
    "        labels = labels % outputs.shape[1]\n",
    "        num_tokens = int(torch.sum(mask).item())\n",
    "        return -torch.sum(outputs[range(outputs.shape[0]), labels]*mask)/num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token-level score function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with idea to use such approach with matrix my groupmate helped me\n",
    "from sklearn.metrics import f1_score\n",
    "metric_types={'TP':0, 'TN':1, 'FP':2, 'FN':3}\n",
    "\n",
    "def micro_f1_score(model, eval_words, eval_tags):\n",
    "    metrics_matrix=np.zeros((len(tag_to_ix), len(metric_types)))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        true_tags = []\n",
    "        preds = []\n",
    "        for batch_data, batch_labels in get_batches(train_words, train_tags):\n",
    "            batch_scores = model(batch_data).view(BATCH_SIZE,-1,9)\n",
    "            batch_labels = batch_labels.numpy()\n",
    "            for batch_sent_index in range(BATCH_SIZE):\n",
    "                scores = batch_scores[batch_sent_index]\n",
    "                predictions = scores.numpy().argmax(axis=1)\n",
    "                for tag_index in range(len(predictions)):\n",
    "                    if batch_data[batch_sent_index, tag_index] == word_to_ix[PAD_WORD]:\n",
    "                        break\n",
    "                    prediction=predictions[tag_index]\n",
    "                    tag = batch_labels[batch_sent_index, tag_index]\n",
    "                    true_tags.append(tag)\n",
    "                    preds.append(prediction)\n",
    "                    if prediction==tag:\n",
    "                        metrics_matrix[:,1]+=1\n",
    "                        metrics_matrix[tag,0]+=1\n",
    "                        metrics_matrix[tag,1]-=1\n",
    "                    else:\n",
    "                        metrics_matrix[:,1]+=1\n",
    "                        metrics_matrix[tag,3]+=1\n",
    "                        metrics_matrix[tag,1]-=1\n",
    "                        metrics_matrix[prediction,2]+=1\n",
    "                        metrics_matrix[prediction,1]-=1\n",
    "    print(\"sklearn f1 micro : \", f1_score(true_tags, preds, average='micro'))\n",
    "    average_precision = sum(metrics_matrix[:,0])/(sum(metrics_matrix[:,0])+sum(metrics_matrix[:,2]))\n",
    "    average_recall = sum(metrics_matrix[:,0])/(sum(metrics_matrix[:,0])+sum(metrics_matrix[:,3]))\n",
    "    f1=2*average_precision*average_recall/(average_precision+average_recall)\n",
    "    f05=1.25*average_precision*average_recall/((0.25*average_precision)+average_recall)\n",
    "    return f1, f05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unison_shuffled(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(words, tags, shuffle=False):\n",
    "    batches = []\n",
    "    order = list(range(len(train_words)))\n",
    "    if shuffle:\n",
    "        words, tags = unison_shuffled(words, tags)\n",
    "    for i in range((len(words)+1) // BATCH_SIZE):\n",
    "        batch_sentences = [prepare_sequence(words[idx], word_to_ix) \n",
    "                           for idx in order[i*BATCH_SIZE:(i+1)*BATCH_SIZE]]\n",
    "        batch_tags = [prepare_sequence(tags[idx], tag_to_ix) \n",
    "                      for idx in order[i*BATCH_SIZE:(i+1)*BATCH_SIZE]]\n",
    "        batch_max_len = max([len(s) for s in batch_sentences])\n",
    "        batch_data = word_to_ix[PAD_WORD]*np.ones((len(batch_sentences), batch_max_len))\n",
    "        batch_labels = -1*np.ones((len(batch_sentences), batch_max_len))\n",
    "        for j in range(len(batch_sentences)):\n",
    "            cur_len = len(batch_sentences[j])\n",
    "            batch_data[j][:cur_len] = batch_sentences[j]\n",
    "            batch_labels[j][:cur_len] = batch_tags[j]\n",
    "        batch_data, batch_labels = torch.LongTensor(batch_data), torch.LongTensor(batch_labels)\n",
    "        batch_data, batch_labels = Variable(batch_data), Variable(batch_labels)\n",
    "        batches.append((batch_data, batch_labels))\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(epochs, model, loss_function, optimizer, train_words, train_tags):\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    for epoch in range(epochs): \n",
    "        print(\"Epoch %d started.\" % (epoch))\n",
    "        model.train()\n",
    "        for batch_data, batch_labels in get_batches(train_words, train_tags, shuffle=True):\n",
    "            model.zero_grad()\n",
    "            tag_scores = model(batch_data)\n",
    "            loss = loss_function(tag_scores, batch_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()  \n",
    "        f1,f05 = micro_f1_score(model, dev_words, dev_tags)\n",
    "        print(f'Epoch {epoch} : f1 loss {f1}')\n",
    "        print(f'Epoch {epoch} : f0.5 loss {f05}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterable approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_iter(epochs, model, loss_function, optimizer, train_words, train_tags):\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    for epoch in range(epochs): \n",
    "        print(\"Epoch %d started.\" % (epoch))\n",
    "        model.train()\n",
    "        order = list(range(len(train_words)))\n",
    "        train_words, train_tags = unison_shuffled(train_words, train_tags)\n",
    "        for i in range((len(train_words))):\n",
    "            model.zero_grad()\n",
    "            sentence = prepare_sequence(train_words[i], word_to_ix)\n",
    "            tags = prepare_sequence(train_tags[i], tag_to_ix)\n",
    "            tag_scores = model(sentence)\n",
    "            loss = loss_function(tag_scores, tags)\n",
    "            loss.backward()\n",
    "            optimizer.step()    \n",
    "        f1,f05 = micro_f1_score(model, dev_words, dev_tags)\n",
    "        print(f'Epoch {epoch} : f1 loss {f1}')\n",
    "        print(f'Epoch {epoch} : f0.5 loss {f05}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 started.\n",
      "sklearn f1 micro :  0.8329869116797582\n",
      "Epoch 0 : f1 loss 0.8329869116797582\n",
      "Epoch 0 : f0.5 loss 0.8329869116797582\n",
      "Epoch 1 started.\n",
      "sklearn f1 micro :  0.8328239865315211\n",
      "Epoch 1 : f1 loss 0.8328239865315211\n",
      "Epoch 1 : f0.5 loss 0.8328239865315211\n",
      "Epoch 2 started.\n",
      "sklearn f1 micro :  0.8327153697660297\n",
      "Epoch 2 : f1 loss 0.8327153697660297\n",
      "Epoch 2 : f0.5 loss 0.8327153697660297\n",
      "Epoch 3 started.\n",
      "sklearn f1 micro :  0.8327351182688463\n",
      "Epoch 3 : f1 loss 0.8327351182688463\n",
      "Epoch 3 : f0.5 loss 0.8327351182688463\n",
      "Epoch 4 started.\n",
      "sklearn f1 micro :  0.8326561242575798\n",
      "Epoch 4 : f1 loss 0.8326561242575798\n",
      "Epoch 4 : f0.5 loss 0.8326561242575797\n"
     ]
    }
   ],
   "source": [
    "model = BiLSTM_Tagger(EMBEDDING_DIM, VOCAB_SIZE, TARGET_SIZE)\n",
    "train_batch(5, model, model.loss_function, optim.SGD(model.parameters(), lr=0.1), train_words, train_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn f1 micro :  0.8326561242575798\n",
      "f1 loss: 0.8326561242575798\n",
      "f0.5 loss: 0.8326561242575798\n"
     ]
    }
   ],
   "source": [
    "f1,f05 = micro_f1_score(model, test_words, test_tags)\n",
    "print(f'f1 loss: {f1}')\n",
    "print(f'f0.5 loss: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lowercase embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 started.\n",
      "sklearn f1 micro :  0.8329869116797582\n",
      "Epoch 0 : f1 loss 0.8329869116797582\n",
      "Epoch 0 : f0.5 loss 0.8329869116797582\n",
      "Epoch 1 started.\n",
      "sklearn f1 micro :  0.8331399625765872\n",
      "Epoch 1 : f1 loss 0.8331399625765872\n",
      "Epoch 1 : f0.5 loss 0.8331399625765872\n",
      "Epoch 2 started.\n",
      "sklearn f1 micro :  0.8356332110571867\n",
      "Epoch 2 : f1 loss 0.8356332110571867\n",
      "Epoch 2 : f0.5 loss 0.8356332110571867\n",
      "Epoch 3 started.\n",
      "sklearn f1 micro :  0.8432906930243351\n",
      "Epoch 3 : f1 loss 0.8432906930243351\n",
      "Epoch 3 : f0.5 loss 0.8432906930243351\n",
      "Epoch 4 started.\n",
      "sklearn f1 micro :  0.8525873007252638\n",
      "Epoch 4 : f1 loss 0.8525873007252638\n",
      "Epoch 4 : f0.5 loss 0.8525873007252638\n"
     ]
    }
   ],
   "source": [
    "model = BiLSTM_Tagger(EMBEDDING_DIM, VOCAB_SIZE, TARGET_SIZE, embedding_type='lower')\n",
    "train_batch(5, model, model.loss_function, optim.SGD(model.parameters(), lr=0.1), train_words, train_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn f1 micro :  0.8525873007252638\n",
      "f1 loss: 0.8525873007252638\n",
      "f0.5 loss: 0.8525873007252638\n"
     ]
    }
   ],
   "source": [
    "f1,f05 = micro_f1_score(model, test_words, test_tags)\n",
    "print(f'f1 loss: {f1}')\n",
    "print(f'f0.5 loss: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 started.\n",
      "sklearn f1 micro :  0.8329869116797582\n",
      "Epoch 0 : f1 loss 0.8329869116797582\n",
      "Epoch 0 : f0.5 loss 0.8329869116797582\n",
      "Epoch 1 started.\n",
      "sklearn f1 micro :  0.833199208085037\n",
      "Epoch 1 : f1 loss 0.833199208085037\n",
      "Epoch 1 : f0.5 loss 0.833199208085037\n",
      "Epoch 2 started.\n",
      "sklearn f1 micro :  0.8366206361980182\n",
      "Epoch 2 : f1 loss 0.8366206361980182\n",
      "Epoch 2 : f0.5 loss 0.8366206361980182\n",
      "Epoch 3 started.\n",
      "sklearn f1 micro :  0.8449298187581153\n",
      "Epoch 3 : f1 loss 0.8449298187581153\n",
      "Epoch 3 : f0.5 loss 0.8449298187581153\n",
      "Epoch 4 started.\n",
      "sklearn f1 micro :  0.8543992258586895\n",
      "Epoch 4 : f1 loss 0.8543992258586895\n",
      "Epoch 4 : f0.5 loss 0.8543992258586895\n"
     ]
    }
   ],
   "source": [
    "model = BiLSTM_Tagger(EMBEDDING_DIM, VOCAB_SIZE, TARGET_SIZE, embedding_type='full')\n",
    "train_batch(5, model, model.loss_function, optim.SGD(model.parameters(), lr=0.1), train_words, train_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn f1 micro :  0.8543992258586895\n",
      "f1 loss: 0.8543992258586895\n",
      "f0.5 loss: 0.8543992258586895\n"
     ]
    }
   ],
   "source": [
    "f1,f05 = micro_f1_score(model, test_words, test_tags)\n",
    "print(f'f1 loss: {f1}')\n",
    "print(f'f0.5 loss: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the best performance in our case gave BiLSTM with full embeddings(search by original word and if not exists, then by lower case). So we received f1 score 0.854 on dev data and test data.\n",
    "\n",
    "Next was the BiLSTM with searching embeddings by lowercase word with dev f1 0.852.\n",
    "\n",
    "And the worst in our case was the BiLSTM with embedding searching strategy only by original word, that gave f1 score 0.832."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
